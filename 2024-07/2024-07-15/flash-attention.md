# Dao-AILab/flash-attention安全风险及SBOM

## 基础信息

项目徽章：

[![Security Status](https://www.murphysec.com/platform3/v31/badge/1812557725058584576.svg)](https://www.murphysec.com/console/report/1737908191462346752/1812557725058584576)

> 点击徽章可查看详细项目安全报告

仓库描述：Fast and memory-efficient exact attention

仓库地址：[https://github.com/Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)

| star：12257 | watch：111 | fork：1087 |
| ----------- | -------------- | ------------ |
| 所有者：Organization | 更新时间：2024-07-14 21:01:05 | 许可证：BSD-3-Clause |
| 最近检测时间： | 组件总数： | 漏洞总数： |




## 漏洞列表

| 漏洞名称 | 漏洞类型 | MPS编号 | CVE编号 | 漏洞等级 |
| ------- | ------ | ------- | ------ | ----- |





## 缺陷组件

| 组件名称 | 版本 | 最小修复版本 | 依赖关系 | 修复建议 |
| -------- | ---- | ------------ | -------- | -------- |





## 许可证风险

| 许可证类型 | 相关组件 | 许可证风险 |
| ---------- | -------- | ---------- |
|MIT|2|Low|




## SBOM清单

| 组件名称 | 组件版本 | 是否直接依赖 | 仓库 |
| -------- | -------- | ------------ | ---- |
|Accuracy||间接依赖|pip|
|GPTBigCodeConfig||间接依赖|pip|
|GPT2Tokenizer||间接依赖|pip|
|get_checkpoint_shard_files||间接依赖|pip|
|GPT2Config||间接依赖|pip|
|opt_config_to_gpt2_config||间接依赖|pip|
|cached_file||间接依赖|pip|
|rearrange||间接依赖|pip|
|Metric||间接依赖|pip|
|FusedMLP||间接依赖|pip|
|Sequence||间接依赖|pip|
|pad_input||间接依赖|pip|
|Version||间接依赖|pip|
|Trainer||间接依赖|pip|
|FusedDense||间接依赖|pip|
|apply_rotary_emb_torch||间接依赖|pip|
|parse||间接依赖|pip|
|List||间接依赖|pip|
|reduce||间接依赖|pip|
|remap_state_dict_hf_opt||间接依赖|pip|
|apply_rotary_emb||间接依赖|pip|
|index_first_axis||间接依赖|pip|
|Callback||间接依赖|pip|
|AutoTokenizer||间接依赖|pip|
|repeat||间接依赖|pip|


------

*此检测报告由墨菲安全提供*

[墨菲安全](www.murphysec.com)